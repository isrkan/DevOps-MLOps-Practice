{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ce9820-a224-45fb-9d3d-0b1dfe8e79f7",
   "metadata": {},
   "source": [
    "# Model customization in MLflow\n",
    "\n",
    "In MLflow, model customization allows us to define custom models with specific behavior beyond what is provided by the standard machine learning libraries. This is particularly useful when we need to integrate special preprocessing steps, post-processing, or custom prediction logic within a model.\n",
    "\n",
    "When customizing a model in MLflow, we typically create a class that inherits from `mlflow.pyfunc.PythonModel`. This class allows us to define how the model should be loaded, how predictions should be made, and any other custom logic we want to include.\n",
    "\n",
    "#### Key concepts in model customization\n",
    "\n",
    "1. **Custom python model**: A Python class that extends `mlflow.pyfunc.PythonModel`. This class serves as a wrapper around our model and defines custom methods for loading the model and making predictions.\n",
    "\n",
    "2. **Essential methods**:\n",
    "   - **`load_context(self, context)`**: This method is responsible for loading the model and any other necessary resources. The `context` parameter provides access to the artifacts logged with the model.\n",
    "   - **`predict(self, context, model_input)`**: This is the core method which defines the custom prediction logic. This method receives `context` (which contains artifacts and other runtime info) and `model_input` (the input data for making predictions). This method returns the predictions.\n",
    "    - While `load_context` and `predict` are essential, we can also add other methods to the custom model class as needed. For example, a `preprocess` method can be included to handle data preprocessing steps before making predictions. This is not required but can be useful for integrating complex preprocessing or feature engineering workflows within the model class.\n",
    "3. **Logging the custom model**: Once we define our custom model class, we can log it to MLflow using `mlflow.pyfunc.log_model`, and then load it using `mlflow.pyfunc.load_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d209e7-5661-493f-8693-f1521d839cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('mlflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6c577-5e68-437a-ac7b-95443aef1b8a",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5562c1bf-155a-4363-a30a-aa95c0a3d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cf1b2-1906-4da5-aabc-3c1f8a014554",
   "metadata": {},
   "source": [
    "### Setting up the experiment\n",
    "\n",
    "We will start by setting up a new MLflow experiment where all runs will be logged. If the experiment does not exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574d6a70-f5e8-4f6d-9ac3-22cc8a491d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/israe/Documents/Codes/Notebooks/mlruns/191308692135956385', creation_time=1724749168776, experiment_id='191308692135956385', last_update_time=1724749168776, lifecycle_stage='active', name='Iris Classification Experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the experiment\n",
    "mlflow.set_experiment(\"Iris Classification Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bf2b0-f951-4f3d-9797-da9e47eaed00",
   "metadata": {},
   "source": [
    "### Defining a custom Python model\n",
    "\n",
    "Let's define a custom model that wraps a scikit-learn models but adds some custom preprocessing before making predictions. We will create a model that applies polynomial feature transformation and scaling, uses an ensemble of classifiers, and applies a threshold to the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f41509-c8b4-4924-bcab-5af6e33d1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom MLflow Python model\n",
    "class CustomModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Load the pre-trained models and scaler artifacts from MLflow\n",
    "        self.poly_features = mlflow.sklearn.load_model(context.artifacts[\"poly_features\"])\n",
    "        self.scaler = mlflow.sklearn.load_model(context.artifacts[\"scaler\"])\n",
    "        self.logistic_regression = mlflow.sklearn.load_model(context.artifacts[\"logistic_regression\"])\n",
    "        self.random_forest = mlflow.sklearn.load_model(context.artifacts[\"random_forest\"])\n",
    "        self.gradient_boosting = mlflow.sklearn.load_model(context.artifacts[\"gradient_boosting\"])\n",
    "        \n",
    "    def preprocess(self, model_input):\n",
    "        # Apply polynomial features and scaling to the input data\n",
    "        poly_input = self.poly_features.transform(model_input)\n",
    "        scaled_input = self.scaler.transform(poly_input)\n",
    "        return scaled_input\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        # Preprocess the input data\n",
    "        preprocessed_input = self.preprocess(model_input)\n",
    "\n",
    "        # Make predictions using the ensemble of pre-trained models\n",
    "        rf_pred = self.random_forest.predict_proba(preprocessed_input)\n",
    "        gb_pred = self.gradient_boosting.predict_proba(preprocessed_input)\n",
    "        lr_pred = self.logistic_regression.predict_proba(preprocessed_input)\n",
    "\n",
    "        # Combine predictions (simple average in this case)\n",
    "        combined_pred = (rf_pred + gb_pred + lr_pred) / 2\n",
    "\n",
    "        # Apply a threshold to binary classification (example: threshold = 0.5)\n",
    "        threshold = 0.5\n",
    "        pred = (combined_pred[:, 1] > threshold).astype(int)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12088243-307c-4ac6-96ac-970231516562",
   "metadata": {},
   "source": [
    "**Custom model definition**:\n",
    "- **`CustomModel` Class**: This class inherits from `mlflow.pyfunc.PythonModel`.\n",
    "- **`load_context`**: This method loads the pre-trained models, polynomial features transformer and scaler from MLflow artifacts. These artifacts are logged as part of the model when we log it with MLflow.\n",
    "- **`preprocess`**: The input data is first transformed using polynomial features and then scaled using the `StandardScaler` before making prediction.\n",
    "- **`predict`**: This method preprocesses the input using the polynomial features transformer and scaler. It then makes predictions using the pre-trained random forest, gradient boosting, and logistic regression models. The predictions from these models are combined (using a simple average in this case), and a threshold is applied to convert probabilities into binary classification. This example illustrates how to encapsulate preprocessing, prediction, and post-processing logic within a custom model.\n",
    "\n",
    "\n",
    "### Training feature transformers and models\n",
    "We will train a set of models and a feature transformer, including polynomial features transformation, scaling, and multiple classification models. We need to train the components before integrating them into a custom model since training each component separately allows us to ensure that each part of the workflow is correctly tuned and validated. This modular approach also makes it easier to debug and test each component independently. In addition, training components separately allow them to be reused across different models or experiments. The custom model class focuses on integrating and using pre-trained components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2c232b-0953-45e8-8d79-6a312a92ef84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the feature transformer, scaler, and models\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
    "logistic_regression = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Create a pipeline for polynomial features and scaling\n",
    "pipeline = make_pipeline(poly_features, scaler)\n",
    "\n",
    "# Fit the pipeline and models on the training data\n",
    "X_train_poly_scaled = pipeline.fit_transform(X_train)\n",
    "random_forest.fit(X_train_poly_scaled, y_train)\n",
    "gradient_boosting.fit(X_train_poly_scaled, y_train)\n",
    "logistic_regression.fit(X_train_poly_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc033cf-727e-4885-b9d3-ae1219865722",
   "metadata": {},
   "source": [
    "### Logging the custom model to MLflow\n",
    "Now, we will log the trained models and feature transformer as artifacts, and wrap them with our custom Python model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e41db7-0f02-4a18-bd1e-503d66c9e1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3b3f0068c84caf832255bcc8d266d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e749e75d954eb3ad53be48576c5594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb1c961b0de49f39019168fd7f255e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add37d9865d3487ca1db64ad2580eea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a6a53a9d3b4c5797b44dbf42578793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: b0516e1c2373403e8b7142ef84ab318c\n"
     ]
    }
   ],
   "source": [
    "# Start a new MLflow run\n",
    "with mlflow.start_run(run_name=\"Custom Model with Scaler Example\") as run:\n",
    "    # Log the polynomial features transformer and retrieve its URI\n",
    "    poly_features_uri = mlflow.sklearn.log_model(poly_features, artifact_path=\"poly_features\").model_uri\n",
    "\n",
    "    # Log the scaler and retrieve its URI\n",
    "    scaler_uri = mlflow.sklearn.log_model(scaler, artifact_path=\"scaler\").model_uri\n",
    "\n",
    "    # Log the random forest model and retrieve its URI\n",
    "    rf_model_uri = mlflow.sklearn.log_model(random_forest, artifact_path=\"random_forest\").model_uri\n",
    "    \n",
    "    # Log the gradient boosting model and retrieve its URI\n",
    "    gb_model_uri = mlflow.sklearn.log_model(gradient_boosting, artifact_path=\"gradient_boosting\").model_uri\n",
    "   \n",
    "    # Log the logistic regression model and retrieve its URI\n",
    "    lr_model_uri = mlflow.sklearn.log_model(logistic_regression, artifact_path=\"logistic_regression\").model_uri\n",
    "    \n",
    "    # Define a custom model with the artifact paths pointing to the logged scaler and logistic regression model\n",
    "    custom_model = CustomModel()\n",
    "    \n",
    "    # Log the custom model\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"custom_model\",\n",
    "        python_model=custom_model,\n",
    "        artifacts={\n",
    "            \"poly_features\": poly_features_uri,\n",
    "            \"scaler\": scaler_uri,\n",
    "            \"random_forest\": rf_model_uri,\n",
    "            \"gradient_boosting\": gb_model_uri,\n",
    "            \"logistic_regression\": lr_model_uri,\n",
    "        },\n",
    "        conda_env=mlflow.pyfunc.get_default_conda_env()\n",
    "    )\n",
    "\n",
    "    # Get the run_id of the current run for later use\n",
    "    run_id = run.info.run_id\n",
    "    # Print the run ID for later use\n",
    "    print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bc7eb-9bee-4715-872f-ec12d795d11e",
   "metadata": {},
   "source": [
    "- **Logging artifacts separately**: First, we log artifacts separately before logging the custom model. By logging each component separately, each artifact is tracked individually in the MLflow experiment run. This provides more granular control over each component, allowing for better versioning, comparison, and tracking.\n",
    "    - **`mlflow.sklearn.log_model`**: Logs the trained component as an artifact. Then, the URI of the logged scaler is captured in `component_uri`. The result will be that the component is stored in MLflowâ€™s artifact repository, complete with its own metadata and a pickle file representing the trained scaler.\n",
    "- **Logging the custom model**: Then, we log the custom Python model with **`mlflow.pyfunc.log_model`**. In addition to the custom model's own metadata and a pickle file for the custom model itself, it also references and includes the metadata and pickle files of the associated artifacts (feature transformers and classification model).\n",
    "    - **Linking artifacts**: The `artifacts` parameter is a dictionary that maps logical names (e.g., \"poly_features\", \"scaler\", etc.) to the URIs of the artifacts previously logged (`component_uri`). When logging a custom model, we use the artifacts parameter to reference already logged components. This requires that the components (e.g., scaler, logistic regression model) are already stored in MLflow's artifact store, thus necessitating separate logging.\n",
    "    - `mlflow.pyfunc.log_model` creates a storage directory which contains a pickle file for the custom model itself and metadata including the logic to load the feature transformer and classification model artifacts. Inside this directory, subdirectories will point to the previously logged feature transformers and classification models, each containing their respective pickle files and their own metadata files.\n",
    "- Conda environment: The `conda_env` parameter specifies the environment needed to run the model, ensuring that the correct dependencies are installed when the model is deployed or reused.\n",
    "\n",
    "\n",
    "### Loading and using the custom model\n",
    "Now that the model is logged, we can load it and use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e95866-6551-4529-adb7-2354639dee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Load the custom model\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/custom_model\")\n",
    "\n",
    "# Make predictions using the custom model on unscaled test data\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Custom model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a2fdd-0106-4bfb-be2c-97ef929a6acf",
   "metadata": {},
   "source": [
    "After logging the custom model, it is loaded using `mlflow.pyfunc.load_model`, and predictions are made on the test set.\n",
    "- **`mlflow.pyfunc.load_model`**: Loads the custom model from the specified URI.\n",
    "- **`predict`**: The `predict` method in the custom model first preprocesses the input data using the polynomial features transformer and scaler. It then makes predictions by combining the outputs from the ensemble of pre-trained models (RandomForestClassifier, GradientBoostingClassifier, and LogisticRegression). The predictions are averaged, and a threshold is applied for binary classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
