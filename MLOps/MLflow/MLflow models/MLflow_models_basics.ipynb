{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4931a70b-a4cc-4b2a-93f0-fef6e13d0223",
   "metadata": {},
   "source": [
    "# MLflow models\n",
    "\n",
    "In this notebook, we will explore MLflow's model component, which provides a unified way to save, load, and deploy machine learning models. MLflow models offer a standardized format for packaging machine learning models that can be used across different platforms and frameworks. An MLflow model is a directory that contains all the necessary files and dependencies required to run our machine learning model, along with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc04be75-6350-43b1-abca-ae68d3c210ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('mlflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae85d3-8062-4555-ba96-297e7fb3e1d8",
   "metadata": {},
   "source": [
    "#### Setting up the experiment\n",
    "This command sets the experiment under which all runs will be recorded. If the experiment doesn’t exist, MLflow will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faef2ea9-0149-4b5e-b94e-7c9a242652e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/israe/Documents/Codes/Notebooks/mlruns/191308692135956385', creation_time=1724749168776, experiment_id='191308692135956385', last_update_time=1724749168776, lifecycle_stage='active', name='Iris Classification Experiment', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the MLflow experiment\n",
    "mlflow.set_experiment(\"Iris Classification Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95f3a4a-8ea3-4fd6-a599-179116478090",
   "metadata": {},
   "source": [
    "#### Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a814ee-8ee5-43c9-a4cf-d7bd5b60f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782da0e-bb88-495d-ac67-c059984d512f",
   "metadata": {},
   "source": [
    "### Model API\n",
    "MLflow provides a simple API for saving, logging, and loading models.\n",
    "\n",
    "#### Saving and logging a model\n",
    "MLflow allows us to both save and log models in a standardized format which is crucial for model management and deployment in production environments.\n",
    "\n",
    "- **Saving a model**: Saving a model in MLflow means storing the model locally on our filesystem in a directory structure that MLflow can recognize and use later. When we save a model using the `mlflow.<framework>.save_model` function, MLflow stores not just the model itself but also all the necessary information required to reconstruct the model later. This includes the model's weights, architecture, and any other necessary dependencies. It is not linked to any MLflow run or experiment, and there is no metadata or versioning associated with it. This approach is useful when we want to store a model for later use in the same environment or manually manage the model files.\n",
    "    - Unlike traditional methods of saving models (e.g., using `joblib` or `pickle`), MLflow’s `save_model` function ensures that the model is saved in a format that includes metadata and dependencies, which makes it easier to deploy the model in different environments. With traditional methods, we would need to manually track the environment and any other dependencies required to run the model.\n",
    "\n",
    "- **Logging a model**: Logging a model in MLflow goes a step beyond saving. When we log a model using the `mlflow.<framework>.log_model` function, MLflow not only saves the model to a local directory but also logs it as an artifact in the current MLflow run. This means that the model is associated with a specific MLflow experiment and run, and all relevant metadata (such as parameters, metrics, and versioning information) is recorded in the MLflow tracking server. Logging a model is particularly useful in production scenarios where we need to track model versions, ensure reproducibility, and potentially deploy models to different environments. The logged model can be easily retrieved later, either for further training, evaluation, or deployment.\n",
    "\n",
    "Let's illustrate these concepts with a simple classification example using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e053d5fb-b0f0-460a-872f-cb0339830f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a simple logistic regression model\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"Logistic Regression Model\")  as run:\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log the accuracy metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Save the model locally\n",
    "    mlflow.sklearn.save_model(model, \"sklearn_model\")\n",
    "\n",
    "    # Log the model to MLflow tracking server\n",
    "    mlflow.sklearn.log_model(model, \"logged_model\")\n",
    "\n",
    "    print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "    # Get the run_id of the current run\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effed39-650e-4abb-833f-bfc9c57b9d9a",
   "metadata": {},
   "source": [
    "- **`mlflow.sklearn.save_model`**: The model is saved to a local directory (`sklearn_model`). This operation only saves the model locally and does not log it to the MLflow server. This is an example of MLflow’s support for scikit-learn models, where the model and all necessary information are stored in a way that MLflow can manage.\n",
    "\n",
    "- **`mlflow.sklearn.log_model`**: The model is logged to the MLflow tracking server. This includes saving the model to a local directory and recording it as an artifact in the current MLflow run. This allows the model to be loaded later from the MLflow tracking server.\n",
    "\n",
    "\n",
    "#### Loading a logged model\n",
    "Once a model is logged, it can be loaded back into memory for inference or further training. Loading a logged model is ideal when we need to work with models that are tracked within an MLflow experiment. This method provides access to the model along with the run’s metadata.\n",
    "- **Limitations**: We need access to the MLflow tracking server where the model was logged. Additionally, this method requires knowing the specific run ID from which we want to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e232143-ceaf-489e-98a2-657a6ca7f18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from the MLflow tracking server using the retrieved run_id\n",
    "loaded_logged_model = mlflow.sklearn.load_model(f\"runs:/{run_id}/logged_model\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "predictions = loaded_logged_model.predict(X_test)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1d4cf-85e2-4f7d-8905-6e2789e15e06",
   "metadata": {},
   "source": [
    "- **Source**: The model is loaded from the MLflow tracking server, using the specific run ID and the artifact path where it was logged. `mlflow.sklearn.load_model` function retrieves the logged model from the MLflow tracking server using the specified `run_id` and artifact path. The loaded model can then be used just like any other scikit-learn model.\n",
    "\n",
    "\n",
    "#### Loading a saved model\n",
    "A saved model is stored locally on our filesystem in a specific directory. We can load this model back into your environment for further use, such as making predictions or continuing training. Loading a saved model is useful when we want to use the model in the same environment where it was originally saved, or when we want to manually manage the model files.\n",
    "- **Limitations**: Since this model is not linked to any MLflow run, we won't have access to the run's metadata (e.g., metrics, parameters) or versioning capabilities. This approach is more isolated and does not leverage MLflow’s tracking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d789c26-5275-4e95-b9fa-05bdc457dedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model from the local directory\n",
    "loaded_saved_model = mlflow.sklearn.load_model(\"sklearn_model\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "predictions = loaded_saved_model.predict(X_test)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48afe4-4bab-4541-b7d6-012f03463639",
   "metadata": {},
   "source": [
    "- **Source**: The model is loaded from the local directory where it was saved using `mlflow.sklearn.save_model`.\n",
    "\n",
    "\n",
    "##### Summary of differences\n",
    "\n",
    "| **Aspect**                     | **Loading from saved model**                                 | **Loading from logged model**                                  |\n",
    "|--------------------------------|--------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Source**                     | Local filesystem                                             | MLflow tracking server                                         |\n",
    "| **Metadata access**            | No access to run metadata                                    | Full access to run metadata (parameters, metrics, tags)        |\n",
    "| **Versioning**                 | No versioning capabilities                                   | Full versioning capabilities                                  |\n",
    "| **Deployment**                 | Manual deployment required                                   | Easier deployment through MLflow’s deployment tools            |\n",
    "| **Environment requirements**   | Must be in the same or similar environment where saved       | Can be loaded in different environments via the MLflow server  |\n",
    "| **Use case**                   | Local use, isolated model management                         | Collaborative projects, production deployment, model comparison |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdcec23-2d04-43c3-8c17-142edbdfa194",
   "metadata": {},
   "source": [
    "#### Storage format \n",
    "\n",
    "When we save or log a model using MLflow, it is stored in a specific directory structure that includes all the necessary files to recreate the model. This directory will contain:\n",
    "\n",
    "- **MLmodel**: This is the main file that defines the model format, metadata about the model, including the format version, the flavor of the model, and any relevant dependencies or signatures.\n",
    "- **model.pkl**: The serialized model artifact (specific to `sklearn` in this case).\n",
    "- **Managing model dependencies**: MLflow automatically captures and manages the dependencies required to run the model. Files for managing dependencies:\n",
    "    - **conda.yaml**: Specifies the Conda environment with the dependencies required to run the model. It is particularly useful when deploying the model in environments where Conda is the preferred package manager. For example:\n",
    "         ```yaml\n",
    "         channels:\n",
    "         - defaults\n",
    "         dependencies:\n",
    "         - python=3.8.5\n",
    "         - scikit-learn=0.24.1\n",
    "         - pip\n",
    "         - pip:\n",
    "           - mlflow\n",
    "         ```\n",
    "         \n",
    "    - **requirements.txt**: This file lists the Python packages required to run the model, which can be installed using pip. For example:\n",
    "         ```\n",
    "         scikit-learn==0.24.1\n",
    "         mlflow==1.19.0\n",
    "         ```\n",
    "\n",
    "    - **python_env.yaml**: Similar to `conda.yaml`, this file lists the Python version and dependencies in a format that can be used to recreate the environment. For example:\n",
    "\n",
    "         ```yaml\n",
    "         python: 3.8.5\n",
    "         dependencies:\n",
    "           - pip==20.2.4\n",
    "           - scikit-learn==0.24.1\n",
    "           - mlflow==1.19.0\n",
    "         ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d84504-73a3-4ef2-92e3-558f76eb8241",
   "metadata": {},
   "source": [
    "### Model signatures and input examples\n",
    "Using model signatures and input examples is a best practice when logging models in MLflow.\n",
    "\n",
    "#### Model signatures\n",
    "A model signature is a description of the model's inputs and outputs. It helps to define the schema of the data that should be passed to the model during inference. The signature includes information about the types and shapes of input features and the types and shapes of the output predictions. During deployment, the model can validate incoming data against its signature, ensuring that the inputs are correctly formatted. The signature also acts as documentation, clearly defining what inputs the model expects, which is particularly useful when sharing models.\n",
    "- **Supported signature types**:\n",
    "  - **DataFrame-based signatures**: MLflow supports signatures for models that take structured data as input, such as pandas DataFrames or numpy arrays. The signature can include various data types like integers, floats, strings, etc.\n",
    "  - **Tensor-based signatures**: MLflow also supports signatures for models that expect tensor inputs, which are common in deep learning models built with frameworks like TensorFlow or PyTorch. Tensor-based signatures capture the shape and type of tensor inputs and outputs, ensuring that the model receives data in the expected format.\n",
    "- **Signature enforcement**: When a model has a signature, MLflow can enforce this schema during model serving to ensure that the input data matches the expected format. This helps to avoid errors and makes the model more robust when deployed. This is not exactly the same as type casting, where data types are automatically converted to match the expected format. Instead, signature enforcement ensures that the input data strictly adheres to the predefined format (e.g., the correct number of features, data types like integers or floats, and even the shape of the input data in tensor-based models).\n",
    "\n",
    "\n",
    "#### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Input examples\n",
    "An input example is a sample input that illustrates the kind of data the model expects. This is particularly useful when sharing models, as it provides immediate insight into the model’s input structure. Input examples are stored alongside the model and can be used to generate example requests in APIs or to validate input data during deployment. They are also useful for testing the model's deployment environment, ensuring that the model can process real-world data as expected.\n",
    "\n",
    "Let’s see how to use model signatures and input examples with MLflow run. We’ll train a simple logistic regression model and log it with both a signature and an input example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee25ea2-4d1c-40ec-8239-6157ad76d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aafd2cdf3ee47789d06328ee39ea773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged with accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DataFrame for better readability in signatures\n",
    "X_train_df = pd.DataFrame(X_train, columns=iris.feature_names)\n",
    "X_test_df = pd.DataFrame(X_test, columns=iris.feature_names)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"Logistic Regression with Signature and Example\"):\n",
    "\n",
    "    # Train a simple logistic regression model\n",
    "    model.fit(X_train_df, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_df)\n",
    "    \n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train_df, y_pred)\n",
    "    \n",
    "    # Define an input example using a subset of the test set\n",
    "    input_example = X_test_df.iloc[:2]\n",
    "\n",
    "    # Log the model with the signature and input example\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        artifact_path=\"logistic_regression_model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    # Log some metrics\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    print(f\"Model logged with accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5425a-61cd-4417-8ac5-f4605bf72c17",
   "metadata": {},
   "source": [
    "- **Model signature**: The signature of the model is inferred using `infer_signature(X_train_df, y_pred)`. This captures the schema of the input dataframe (`X_train_df`) and the output predictions (`y_pred`). The signature will ensure that any input data provided to the model in the future adheres to the expected format and data types.\n",
    "- **Input example**: The input example is defined using a small subset of the test data (`X_test_df.iloc[:2]`). This example is logged alongside the model and serves as a reference for what kind of input the model expects.\n",
    "- **Logging the model**: The model is logged using `mlflow.sklearn.log_model`, where we specify the model itself, the `artifact_path` (which is where the model and related files will be stored), the `signature`, and the `input_example`.\n",
    "\n",
    "**Storage format changes after logging signatures and input examples**\n",
    "\n",
    "When we log a model with signatures and input examples, additional files and information are stored to capture these details. Here's what gets added:\n",
    "- **`input_example.json`**: This file contains an example of the input data that the model expects. It helps users or systems interacting with the model understand the format and structure of the input data. For instance, if our model expects a pandas DataFrame, this file will store a sample DataFrame in a JSON format.\n",
    "- **`serving_input_payload.json`**: This file stores the input data in a format suitable for model serving. It's typically used during the deployment phase to ensure that the inputs are correctly formatted according to the model's signature. It can serve as a reference for how the input should look when the model is deployed as a REST API.\n",
    "- **Extra information in the `MLmodel` file**: The `MLmodel` file is a configuration file that contains metadata about the logged model. When we log a model with a signature and input examples, additional information is added to this file, such as:\n",
    "  - **`saved_input_example_info`**: This section contains metadata about the saved input example, including:\n",
    "      - **`artifact_path`**: The path to the `input_example.json` file within the model's directory.\n",
    "      - **`pandas_orient`**: The format in which the pandas DataFrame is serialized (e.g., 'split', 'records').\n",
    "      - **`serving_input_path`**: The path to the `serving_input_payload.json` file.\n",
    "      - **`type`**: The type of input data (e.g., 'dataframe' in this case).\n",
    "  - **`signature`**: This section describes the expected inputs and outputs of the model. It lists:\n",
    "      - **`inputs`**: A detailed schema of the expected input, including the data types and feature names.\n",
    "      - **`outputs`**: A schema of the expected output, including the data type and shape (e.g., a tensor with a specific shape)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62682a39-495a-4bd3-a4aa-6df1bb1af929",
   "metadata": {},
   "source": [
    "### Model flavors in MLflow\n",
    "MLflow model flavors are a key concept that makes it easier to work with machine learning models across different frameworks and tools. A flavor in MLflow refers to a specific format or interface for saving and loading models. Each flavor corresponds to a particular machine learning library or framework, and it allows models saved in that flavor to be easily loaded and used by the same or compatible libraries.\n",
    "- Interoperability: Flavors enable a model to be used in different environments and frameworks without needing to modify the model itself. For example, a model trained with scikit-learn can be loaded and used in an environment that supports scikit-learn, regardless of where the model was trained.\n",
    "- Deployment flexibility: When we save a model in MLflow, it can be saved with multiple flavors, making it easier to deploy the model in different contexts. For instance, a model saved with a Python flavor can be deployed in a Python environment, while the same model saved with an MLeap flavor can be deployed in a Java environment.\n",
    "- Consistency: Flavors provide a standardized way of saving and loading models, ensuring that the process is consistent across different frameworks.\n",
    "\n",
    "##### Commonly used flavors\n",
    "- **Python Function (pyfunc)**: The most general flavor that supports models written in Python. This is a catch-all flavor that allows models to be deployed as a generic Python function, regardless of the original framework used. By wrapping the model in a Python function interface, it becomes possible to use the model with different libraries and environments.\n",
    "- **Scikit-learn**: A specific flavor for models trained with the scikit-learn library. It logs the model specifically for use with the scikit-learn framework. This flavor saves the model in a format that is natively supported by scikit-learn. The logged model can be directly loaded and used with scikit-learn's API. It includes information specific to scikit-learn models, such as hyperparameters and training details.\n",
    "- **TensorFlow**: A flavor for models trained with TensorFlow. Models logged with this flavor are saved in a way that is compatible with TensorFlow’s APIs, allowing for seamless integration and further training within TensorFlow.\n",
    "- **PyTorch**: A flavor for models trained with PyTorch.\n",
    "- **Spark MLlib**: A flavor for models trained with Spark MLlib.\n",
    "- **MLeap**: A flavor that enables models to be serialized into a format that can be served in a JVM environment, often used with Spark.\n",
    "\n",
    "A model can only be logged with the flavor corresponding to the framework it was trained in. For example, a model trained in TensorFlow can only be logged with the TensorFlow flavor. You cannot log a TensorFlow model with a PyTorch flavor because these flavors are tightly coupled with their respective frameworks. However, the same model (regardless of whether it was trained in TensorFlow, PyTorch, or another framework) can additionally be logged with the `pyfunc` flavor. This allows the model to be used in a more generalized way across different Python environments. If we log the model only with the TensorFlow flavor, it requires the TensorFlow framework to be available in the environment where the model will be used or deployed. However, by logging it with the pyfunc flavor, the model can be loaded and used in any Python environment that may not have TensorFlow installed but still needs to perform inference using the model.\n",
    "\n",
    "Let's go through an example where we save and load a model using multiple flavors in MLflow. We'll train a simple scikit-learn model, log it with different flavors, and demonstrate how to load the model using a specific flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab972b0-d725-4ad4-9b1f-4be099401d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849557dd3ebd43d8bf37ada85ae24bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged with multiple flavors!\n",
      "Loaded model with sklearn flavor: LogisticRegression(max_iter=200)\n",
      "Loaded model with pyfunc flavor: mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: pyfunc_model\n",
      "  flavor: mlflow.pyfunc.model\n",
      "  run_id: f92c7c36f4bd42feb3121ffc7e418eaa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a simple logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"Logistic Regression with Flavors\") as run:\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log the model with scikit-learn flavor\n",
    "    mlflow.sklearn.log_model(model, \"sklearn_model\")\n",
    "\n",
    "    # Log the model with pyfunc flavor (default)\n",
    "    mlflow.pyfunc.log_model(\"pyfunc_model\", python_model=mlflow.pyfunc.PythonModel(), artifacts={\"model\": \"sklearn_model\"})\n",
    "\n",
    "    print(\"Model logged with multiple flavors!\")\n",
    "\n",
    "# Get the last run_id\n",
    "run_id = run.info.run_id\n",
    "\n",
    "# Loading the model using scikit-learn flavor\n",
    "loaded_sklearn_model = mlflow.sklearn.load_model(f\"runs:/{run_id}/sklearn_model\")\n",
    "print(f\"Loaded model with sklearn flavor: {loaded_sklearn_model}\")\n",
    "\n",
    "# Loading the model using pyfunc flavor\n",
    "loaded_pyfunc_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/pyfunc_model\")\n",
    "print(f\"Loaded model with pyfunc flavor: {loaded_pyfunc_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c652e24-617b-4faf-ae6c-f82fa4795001",
   "metadata": {},
   "source": [
    "- **Logging the model with different flavors**:\n",
    "   - **Scikit-learn flavor** (`mlflow.sklearn.log_model`): This command logs the model using the scikit-learn flavor. This means the model will be saved in a format that can be directly loaded and used by scikit-learn.\n",
    "   - **Python function (pyfunc) flavor** (`mlflow.pyfunc.log_model`): This command logs the model as a generic Python function. The `pyfunc` flavor allows the model to be loaded and used as a general Python function, making it compatible with any Python environment.\n",
    "\n",
    "- **Loading the model with specific flavors**:\n",
    "   - **Scikit-learn Flavor** (`mlflow.sklearn.load_model`): This loads the model using the scikit-learn flavor, allowing us to use it just like any other scikit-learn model.\n",
    "   - **Python function (pyfunc) flavor** (`mlflow.pyfunc.load_model`): This loads the model using the pyfunc flavor. The model can now be used as a generic Python function, making it more versatile. Once loaded, the model is accessible as a Python object with a `predict` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f49368-d8a4-4026-8185-b60c109b4343",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "Model evaluation involves assessing the performance of a trained model using various metrics and visualizations to understand how well the model is likely to perform on unseen data. MLflow provides built-in tools for evaluating models, making it easier to compare different models and select the best one. When you evaluate a model using MLflow, the following gets logged:\n",
    "- **Metrics**: Quantitative evaluation metrics like accuracy, precision, recall, etc.\n",
    "- **Artifacts**: Visual artifacts like confusion matrices, ROC and precision-recall curves, feature importance plots, etc., that help in understanding the model’s performance.\n",
    "- **Model explainability**: If enabled, additional explainability tools such as SHAP values might be logged to help in interpreting the model’s predictions.\n",
    "\n",
    "MLflow's `mlflow.evaluate` function automatically computes metrics and generates evaluation artifacts for a given model and dataset. MLflow supports evaluating models logged with different flavors, including `pyfunc`, `sklearn`, `tensorflow`, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bc3cb3-4082-4a26-970e-0a72f7b751c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'score': 1.0, 'example_count': 30, 'accuracy_score': 1.0, 'recall_score': 1.0, 'precision_score': 1.0, 'f1_score': 1.0, 'log_loss': 0.11128299003032528, 'roc_auc': 1.0}\n",
      "Evaluation artifacts: dict_keys(['roc_curve_plot', 'precision_recall_curve_plot', 'per_class_metrics', 'confusion_matrix', 'shap_beeswarm_plot', 'shap_summary_plot', 'shap_feature_importance_plot'])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run(run_name=\"Logistic Regression Model Evaluation\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Log the model\n",
    "mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n",
    "\n",
    "# Evaluate the model using mlflow.evaluate\n",
    "evaluation_results = mlflow.evaluate(\n",
    "    model=\"runs:/{}/logistic_regression_model\".format(mlflow.active_run().info.run_id),  # Model URI as string\n",
    "    data=X_test, # Test data\n",
    "    targets=y_test, # True labels\n",
    "    model_type=\"classifier\", # Model type as classifier\n",
    "    evaluators=[\"default\"], # Default evaluator\n",
    "    evaluator_config={\"default\": {\"log_model_explainability\": True}} # Additional configurations\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Evaluation metrics: {evaluation_results.metrics}\")\n",
    "print(f\"Evaluation artifacts: {evaluation_results.artifacts.keys()}\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c772b3e-2580-40d6-9c34-241edc88dd7f",
   "metadata": {},
   "source": [
    "**Evaluating the model**: We use `mlflow.evaluate` to evaluate the model on the test data, and automatically log the results to the MLflow Tracking server.\n",
    "-  **`model`**: The model to be evaluated, which was logged earlier. It points to the logged model using its URI with the appropriate run ID. This is a string that MLflow can use to locate and load the model internally during the evaluation process. \n",
    "- **`data`**: The input data (in this case, `X_test`) used for evaluation. This can be a numpy array, pandas DataFrame, or other supported data formats depending on the model type.\n",
    "- **`targets`**: The true labels (`y_test`) corresponding to the input data.\n",
    "- **`model_type`**: Specifies the type of model being evaluated, such as `\"classifier\"` for classification models or `\"regressor\"` for regression models. This ensures that appropriate metrics and evaluation techniques are applied.\n",
    "- **`evaluators`**: A list of evaluators to use during evaluation. In this case, we use `\"default\"`, which automatically selects appropriate evaluators based on the model type. Other options might include custom evaluators or specialized evaluators like `\"mleap\"` for certain Spark models.\n",
    "- **`evaluator_config`**: A dictionary for configuring the evaluators. In this example, we enable `\"log_model_explainability\"` by setting it to `True`, which allows logging of explainability artifacts like SHAP values (nned to insure that `shap` library is installed).\n",
    "- **Additional Parameters**:\n",
    "  - **`batch_size`**: If the model supports batch predictions, this parameter can be used to specify the batch size during evaluation.\n",
    "  - **`custom_metrics`**: A dictionary of custom metric functions. This allows us to compute and log additional metrics or generate specialized artifacts beyond the default ones provided by MLflow that are specific to your use case.\n",
    "\n",
    "\n",
    "**Evaluation results**: The results of the evaluation include metrics and artifacts. Metrics might include accuracy, precision, recall, etc., while artifacts could include visualizations and tables like confusion matrices or feature importance plots. The evaluation results are stored in a dictionary, which includes metrics and paths to any generated artifacts.\n",
    "\n",
    "#### Evaluating a model with a baseline model\n",
    "In addition to evaluating a model's performance on its own, MLflow allows us to compare it against a baseline model. This comparison helps in understanding how much better (or worse) the candidate model is compared to a simple or well-understood baseline. Using the `mlflow.evaluate` function, we can also apply validation thresholds, which can enforce certain performance criteria for the model. This approach to evaluation ensures that your model is not only accurate but also meaningfully better than a basic model.\n",
    "- **Baseline model**: The baseline model is typically a simple model that represents the minimum performance we would expect. It serves as a reference point to judge the candidate model's effectiveness. For instance, in a classification problem, a `DummyClassifier` could be used as the baseline model.\n",
    "- **Validation thresholds**: These thresholds ensure that the candidate model not only performs well but also significantly outperforms the baseline. If the candidate model fails to meet these criteria, it might not be considered suitable for production, even if it has a high overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b99e6d-4f54-44f6-a3dc-6dfc0658f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'score': 1.0, 'example_count': 30, 'accuracy_score': 1.0, 'recall_score': 1.0, 'precision_score': 1.0, 'f1_score': 1.0, 'log_loss': 0.11128299003032528, 'roc_auc': 1.0}\n",
      "Evaluation artifacts: dict_keys(['roc_curve_plot', 'precision_recall_curve_plot', 'per_class_metrics', 'confusion_matrix'])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the candidate model (Logistic Regression)\n",
    "candidate_model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Initialize the baseline model (Dummy Classifier)\n",
    "baseline_model = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"Model Evaluation with Baseline Comparison\") as run:\n",
    "    # Train the candidate model\n",
    "    candidate_model.fit(X_train, y_train)\n",
    "\n",
    "    # Train the baseline model\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "\n",
    "    # Log the candidate model\n",
    "    candidate_model_uri = mlflow.sklearn.log_model(candidate_model, \"candidate_model\").model_uri\n",
    "\n",
    "    # Log the baseline model\n",
    "    baseline_model_uri = mlflow.sklearn.log_model(baseline_model, \"baseline_model\").model_uri\n",
    "\n",
    "    # Define validation thresholds\n",
    "    thresholds = {\n",
    "        \"accuracy_score\": mlflow.models.MetricThreshold(\n",
    "            threshold=0.8,  # Candidate model accuracy should be >= 0.8\n",
    "            min_absolute_change=0.05,  # Accuracy should be at least 0.05 greater than baseline accuracy\n",
    "            min_relative_change=0.05,  # Accuracy should be at least 5% greater than baseline accuracy\n",
    "            greater_is_better=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Evaluate the candidate model against the baseline model\n",
    "    evaluation_results = mlflow.evaluate(\n",
    "        model=candidate_model_uri,\n",
    "        data=X_test, # Test data\n",
    "        targets=y_test, # True labels\n",
    "        model_type=\"classifier\",\n",
    "        validation_thresholds=thresholds,\n",
    "        baseline_model=baseline_model_uri,\n",
    "        evaluators=[\"default\"], # Default evaluator\n",
    "    )\n",
    "\n",
    "    # Print the evaluation results\n",
    "    print(f\"Evaluation metrics: {evaluation_results.metrics}\")\n",
    "    print(f\"Evaluation artifacts: {evaluation_results.artifacts.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320acdb7-bc7f-4d7d-938d-2dde2dfc1a7a",
   "metadata": {},
   "source": [
    "- **`model`**: The candidate model URI, which is the model we want to evaluate against a baseline. This URI is obtained after logging the model with MLflow.\n",
    "- **`data`**: The evaluation dataset, which includes the features (`X_test`).\n",
    "- **`targets`**: Specifies the true labels (`y_test`)\n",
    "- **`model_type`**: Indicates the type of model, `\"classifier\"` in this case, since we are working with a classification problem.\n",
    "- **`validation_thresholds`**: This is a dictionary that defines thresholds and validation criteria for the evaluation metrics. For example, the candidate model's accuracy must be at least 0.8, and it should outperform the baseline model by at least 0.05 in both absolute and relative terms.\n",
    "- **`baseline_model`**: The URI of the baseline model, which is a simple model that serves as a benchmark. In this example, it's a `DummyClassifier` that predicts uniformly at random.\n",
    "- **`evaluators`**: Specifies the evaluators to use. `\"default\"` is used to automatically apply appropriate evaluation metrics and generate artifacts.\n",
    "- **`evaluator_config`**: Allows additional configurations, such as enabling model explainability tools (e.g., SHAP values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
