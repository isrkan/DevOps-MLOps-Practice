{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a4120b-ce04-489b-8a9b-2aeff7487c14",
   "metadata": {},
   "source": [
    "# Introduction to MLflow tracking \n",
    "\n",
    "MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, and deployment. In this notebook, we will walk through the process of using MLflow to track machine learning experiments. We will use the Iris dataset and perform dimensionality reduction followed by clustering, logging all relevant information to MLflow.\n",
    "\n",
    "MLflow's Tracking component allows us to log and query experiments, comparing results, and ensuring reproducibility. It records key information such as:\n",
    "- **Parameters:** Input values or configurations used in the experiments, like hyperparameters.\n",
    "- **Metrics:** Performance measurements, such as accuracy or loss, that help evaluate the model.\n",
    "- **Artifacts:** Output files or models generated during the experiment, like plots, model files, or logs.\n",
    "- **Tags:** Labels to help organize and filter experiments, making it easier to search and manage runs.ucibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79886ed5-608a-453e-8645-ae0dbc4a2d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import DBSCAN, OPTICS, Birch\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55196a-4c48-470c-8b2d-e064161794cf",
   "metadata": {},
   "source": [
    "### Setting up the experiment\n",
    "\n",
    "First, we need to specify the experiment we want to track. An experiment in MLflow is a collection of related runs. We use `mlflow.set_experiment()` to specify the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de591174-58b2-4bc6-97fb-7d64cc82f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/30 13:38:31 INFO mlflow.tracking.fluent: Experiment with name 'Iris Dataset Clustering' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the experiment\n",
    "mlflow_experiment = mlflow.set_experiment(\"Iris Dataset Clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a0529-31bf-43da-8a26-9438c3a48649",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **`mlflow.set_experiment(experiment_name)`**: This sets the specified experiment as the active experiment where all subsequent runs will be logged. If the experiment doesn't exist, it will be created with the given name. We can also set an experiment using its ID with the `experiment_id parameter`. However, we must specify either the name or the ID, not both. If we use the experiment ID and it doesn't exist, an exception will be raised. The return value is stored in the `mlflow_experiment` object with details about the experiment.\n",
    "\n",
    "#### Adding tags and description to experiment (optional)\n",
    "Once the experiment is created or set, we can add tags and a description to help organize and provide additional context for the experiment. Tags are key-value pairs that provide metadata about the experiment, making it easier to search and filter experiments in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54756437-037d-4343-a786-cc1b545ac1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"This experiment focuses on clustering the Iris dataset to identify different species based on their features.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the runs that will be in this experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"Iris Dataset Analysis\",\n",
    "    \"analysis_type\": \"clustering\",\n",
    "    \"team\": \"data-science\",\n",
    "    \"model_type\": \"unsupervised\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Set the experiment tags\n",
    "mlflow.set_experiment_tags(experiment_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe489c69-c31f-434b-9202-0b89e2b5b859",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **`mlflow.set_experiment_tags(experiment_tags)`**: This function applies the specified tags to the active experiment. Tags help in organizing and searching experiments by their metadata in the MLflow UI.\n",
    "    - **`experiment_tags`**: A dictionary of tags where each key-value pair describes characteristics of the experiment. Tags can include project name, analysis type, team, model type and more. The key `mlflow.note.content` is used to include the experiment description within the tags.\n",
    "    - **`experiment_description`**: A string that provides a summary of what the experiment is about. This description appears in the MLflow UI for easy reference.\n",
    "\n",
    "### Defining and running the experiment\n",
    "Now let's define the function `run_experiment` that will perform dimensionality reduction and clustering, and log all relevant information to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff7a027-a51a-48a1-bab5-84a03626c81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(dr_method, cluster_method, dataset_name, data, labels, n_components=2):\n",
    "    run_name = f\"{dr_method}-{cluster_method}\"\n",
    "    \n",
    "    # Start the run explicitly\n",
    "    mlflow.start_run(run_name=run_name)\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if dr_method == 'PCA':\n",
    "        dr_model = PCA(n_components=n_components)\n",
    "    elif dr_method == 't-SNE':\n",
    "        dr_model = TSNE(n_components=n_components)\n",
    "    elif dr_method == 'UMAP':\n",
    "        dr_model = umap.UMAP(n_components=n_components)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dimensionality reduction method\")\n",
    "\n",
    "    reduced_data = dr_model.fit_transform(data)\n",
    "\n",
    "    # Clustering\n",
    "    if cluster_method == 'DBSCAN':\n",
    "        cluster_model = DBSCAN()\n",
    "    elif cluster_method == 'OPTICS':\n",
    "        cluster_model = OPTICS()\n",
    "    elif cluster_method == 'BIRCH':\n",
    "        cluster_model = Birch()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported clustering method\")\n",
    "\n",
    "    clusters = cluster_model.fit_predict(reduced_data)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette_avg = silhouette_score(reduced_data, clusters)\n",
    "    davies_bouldin_avg = davies_bouldin_score(reduced_data, clusters)\n",
    "\n",
    "    # Log parameters\n",
    "    params = {\n",
    "        \"dimensionality_reduction\": dr_method,\n",
    "        \"clustering_method\": cluster_method,\n",
    "        \"n_components\": n_components,\n",
    "        \"dataset_name\": dataset_name\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        \"silhouette_score\": silhouette_avg,\n",
    "        \"davies_bouldin_score\": davies_bouldin_avg\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(dr_model, \"dimensionality_reduction_model\")\n",
    "    mlflow.sklearn.log_model(cluster_model, \"clustering_model\")\n",
    "\n",
    "    # Plot and log the clustering result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='k')\n",
    "    plt.title(f'{dr_method} + {cluster_method} Clustering')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    #plot_path = f'plots/{dr_method}_{cluster_method}.png'\n",
    "    #plt.savefig(plot_path)\n",
    "    mlflow.log_figure(plt.gcf(), f\"{dr_method}_{cluster_method}_plot.png\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    #mlflow.log_artifact(plot_path)\n",
    "\n",
    "    # Set a description for the run\n",
    "    run_description = (\n",
    "        f\"This run uses {dr_method} for dimensionality reduction and {cluster_method} for clustering on the {dataset_name} dataset.\"\n",
    "    )\n",
    "    \n",
    "    # Set tags\n",
    "    tags = {\n",
    "        \"project\": \"Iris Clustering\",\n",
    "        \"team\": \"Data Science\",\n",
    "        \"developer\": \"Israel\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"dim_reduction\": dr_method,\n",
    "        \"clustering\": cluster_method,\n",
    "        \"mlflow.note.content\": run_description\n",
    "    }\n",
    "    mlflow.set_tags(tags)\n",
    "    \n",
    "    # End the run explicitly\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf70bf-29ef-4276-8e32-9bf1af2ad555",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **`mlflow.start_run(run_name=run_name)`**: Starts an MLflow run with a specified name. This is where all tracking information (parameters, metrics, artifacts) will be logged, and it will be associated with the experiment set earlier. Parameters:\n",
    "  - **run_id (str, optional):** If specified, resumes the run with the given ID instead of creating a new run.\n",
    "  - **experiment_id (str, optional):** Specifies the experiment under which to create the run. If not specified, the active experiment or the default experiment is used.\n",
    "  - **run_name (str, optional):** A descriptive name for the run. It can make it easier to identify runs in the UI.\n",
    "  - **nested (bool, optional):** If `True`, starts a nested run under the current active run.\n",
    "\n",
    "- **`mlflow.log_params(params)`**: Logs a dictionary of parameters for the current run to MLflow. Parameters are key-value pairs that represent the hyperparameters or other configurations of the experiment.\n",
    "- **`mlflow.log_metrics(metrics)`**: Logs a dictionary of metrics for the current run to MLflow. Metrics are key-value pairs that represent the performance or outcome of the experiment.\n",
    "- **`mlflow.set_tags(tags)`**: Sets multiple tags for the current run. Tags are useful for filtering and organizing runs in MLflow.\n",
    "    - **`mlflow.note.content`**: A description of the run in the MLflow UI, explaining the methods and dataset used.\n",
    "- **`mlflow.log_artifact()`**: Logs a local file or directory as an artifact for the current run. Artifacts are typically output files such as model files, plots, or other files generated during the run. Parameters:\n",
    "  - **local_path (str):** The path to the file or directory to log as an artifact.\n",
    "  - **artifact_path (str, optional):** If provided, the path within the runâ€™s artifact directory to log the artifact to. Defaults to logging at the root level.\n",
    "\n",
    "- **`mlflow.log_figure(plt.gcf(), \"plot_name.png\")`**: Logs the current Matplotlib figure directly to MLflow as an artifact. The figure does not need to be saved to a file beforehand.\n",
    "- **`mlflow.sklearn.log_model(sk_model)`**: Logs a Scikit-learn model as an artifact for the current run. Parameters:\n",
    "  - **sk_model (object):** The Scikit-learn model to log.\n",
    "  - **artifact_path (str):** The directory under which to log the model.\n",
    "  - **serialization_format (str, optional):** The format to use for serializing the model (default is `'cloudpickle'`).\n",
    "  - **registered_model_name (str, optional):** If provided, this will register the model under the given name in the model registry.\n",
    "\n",
    "- **`mlflow.end_run()`**: Ends the current active run, ensuring that all the logged data is saved. Parameters:\n",
    "  - **status (str, optional):** The run status, such as `'FINISHED'`, `'FAILED'`, or `'KILLED'`. Defaults to `'FINISHED'`.\n",
    "\n",
    "### Running the experiments\n",
    "With the function defined, we can now run experiments with different combinations of dimensionality reduction and clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd53687-1d18-4ea8-97e1-46fe889eae94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/30 13:38:31 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:38:39 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:39 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:38:42 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:43 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:38:46 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:46 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:38:49 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:50 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:38:53 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:56 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:38:57 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:00 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:00 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:04 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:05 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:08 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:08 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:11 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:12 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:15 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:19 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:31 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:40 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:40 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:43 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:47 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:55 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:39:55 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:39:59 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:40:02 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2024/08/30 13:40:11 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n",
      "2024/08/30 13:40:16 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "dataset_name = \"iris\"\n",
    "\n",
    "# Define your methods\n",
    "dr_methods = ['PCA', 't-SNE', 'UMAP']\n",
    "cluster_methods = ['DBSCAN', 'OPTICS', 'BIRCH']\n",
    "\n",
    "# Run experiments\n",
    "for dr_method in dr_methods:  # Loop through each dimensionality reduction method\n",
    "    for cluster_method in cluster_methods:  # Loop through each clustering method\n",
    "        run_experiment(dr_method, cluster_method, dataset_name, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff97aa-c2ab-4ccd-a51f-e242edbc3761",
   "metadata": {},
   "source": [
    "### Access the MLflow UI\n",
    "After running the code and logging experiments with MLflow, we might want to explore the logged data, parameters, metrics, and artifacts through the MLflow UI. We can start the MLflow UI by running the following command in the terminal:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "If the UI is not showing the experiment, we can start the MLflow UI with an explicit path to the correct `mlruns` directory:\n",
    "\n",
    "```bash\n",
    "mlflow ui --backend-store-uri \"file:///absolute/path/to/mlruns\"\n",
    "```\n",
    "\n",
    "Replace `/absolute/path/to/mlruns` with the correct path to the `mlruns` directory.\n",
    "\n",
    "Once the MLflow server is running, we can access the MLflow UI by opening the web browser and navigating to:\n",
    "```\n",
    "http://localhost:5000\n",
    "```\n",
    "\n",
    "When we are done using the MLflow UI, we can stop the server by pressing `Ctrl+C` in the terminal.\n",
    "\n",
    "##### Explore the Experiments\n",
    "In the MLflow UI, we can do the following:\n",
    "- **View experiments**: On the main page, we will see a list of all experiments. Click on an experiment to view the associated runs.\n",
    "- **Inspect runs**: For each run, we can inspect logged parameters, metrics, artifacts, and more.\n",
    "- **Compare runs**: Select multiple runs to compare them side-by-side, which is useful for evaluating different models and hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
